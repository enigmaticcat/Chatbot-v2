"""
File ch√≠nh ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Chatbot AI
Ch·ª©c nƒÉng: 
- T·∫°o giao di·ªán web v·ªõi Streamlit
- X·ª≠ l√Ω t∆∞∆°ng t√°c chat v·ªõi ng∆∞·ªùi d√πng
- K·∫øt n·ªëi v·ªõi AI model ƒë·ªÉ tr·∫£ l·ªùi (Gemini & Ollama)
"""

# === IMPORT C√ÅC TH∆Ø VI·ªÜN C·∫¶N THI·∫æT ===
import streamlit as st  # Th∆∞ vi·ªán t·∫°o giao di·ªán web
from dotenv import load_dotenv 
import asyncio, nest_asyncio
try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())
nest_asyncio.apply()
from seed_data import seed_milvus, seed_milvus_live  # H√†m x·ª≠ l√Ω d·ªØ li·ªáu
from agent import get_retriever, get_llm_and_agent
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

# === THI·∫æT L·∫¨P GIAO DI·ªÜN TRANG WEB ===
def setup_page():
    """
    C·∫•u h√¨nh trang web c∆° b·∫£n
    """
    st.set_page_config(
        page_title="AI Assistant",  # Ti√™u ƒë·ªÅ tab tr√¨nh duy·ªát
        page_icon="üí¨",  # Icon tab
        layout="wide"  # Giao di·ªán r·ªông
    )

# === KH·ªûI T·∫†O ·ª®NG D·ª§NG ===
def initialize_app():
    """
    Kh·ªüi t·∫°o c√°c c√†i ƒë·∫∑t c·∫ßn thi·∫øt:
    - ƒê·ªçc file .env ch·ª©a API key
    - C·∫•u h√¨nh trang web
    """
    load_dotenv()  # ƒê·ªçc API key t·ª´ file .env
    setup_page()  # Thi·∫øt l·∫≠p giao di·ªán

# === THANH C√îNG C·ª§ B√äN TR√ÅI ===
def setup_sidebar():
    """
    T·∫°o thanh c√¥ng c·ª• b√™n tr√°i v·ªõi c√°c t√πy ch·ªçn
    """
    with st.sidebar:
        st.title("‚öôÔ∏è C·∫•u h√¨nh")
        
        # Ph·∫ßn 1: Ch·ªçn Embeddings Model
        st.header("üî§ Embeddings Model")
        embeddings_choice = st.radio(
            "Ch·ªçn Embeddings Model:",
            ["Gemini", "Ollama"]
        )
        use_ollama_embeddings = (embeddings_choice == "Ollama")
        
        # Ph·∫ßn 2: C·∫•u h√¨nh Data
        st.header("üìö Ngu·ªìn d·ªØ li·ªáu")
        data_source = st.radio(
            "Ch·ªçn ngu·ªìn d·ªØ li·ªáu:",
            ["File Local", "URL tr·ª±c ti·∫øp"]
        )
        
        # X·ª≠ l√Ω ngu·ªìn d·ªØ li·ªáu d·ª±a tr√™n embeddings ƒë√£ ch·ªçn
        if data_source == "File Local":
            handle_local_file(use_ollama_embeddings)
        else:
            handle_url_input(use_ollama_embeddings)
            
        # Th√™m ph·∫ßn ch·ªçn collection ƒë·ªÉ query
        st.header("üîç Collection ƒë·ªÉ truy v·∫•n")
        collection_to_query = st.text_input(
            "Nh·∫≠p t√™n collection c·∫ßn truy v·∫•n:",
            "data_test",
            help="Nh·∫≠p t√™n collection b·∫°n mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin"
        )
        
        # Ph·∫ßn 3: Ch·ªçn Model ƒë·ªÉ tr·∫£ l·ªùi
        st.header("ü§ñ Model AI")
        model_choice = st.radio(
            "Ch·ªçn AI Model ƒë·ªÉ tr·∫£ l·ªùi:",
            ["Google Gemini", "Ollama (Local)"]
        )
        
        # Ph·∫ßn 4: C·∫•u h√¨nh Ollama (n·∫øu ch·ªçn Ollama)
        if model_choice == "Ollama (Local)":
            st.subheader("ü¶ô C·∫•u h√¨nh Ollama")
            ollama_model = st.selectbox(
                "Ch·ªçn model Ollama:",
                ["llama3.2:3b", "llama3.2:1b", "llama3.1:8b", "qwen2.5:7b", "mistral:7b"],
                help="ƒê·∫£m b·∫£o model ƒë√£ ƒë∆∞·ª£c pull v·ªÅ local"
            )
            
            ollama_url = st.text_input(
                "URL Ollama server:",
                "http://localhost:11434",
                help="URL c·ªßa Ollama server"
            )
            
            # Ki·ªÉm tra k·∫øt n·ªëi Ollama
            if st.button("üîç Ki·ªÉm tra k·∫øt n·ªëi Ollama"):
                check_ollama_connection(ollama_url, ollama_model)
        else:
            ollama_model = None
            ollama_url = None
        
        return model_choice, collection_to_query, ollama_model, ollama_url

def check_ollama_connection(ollama_url, model_name):
    """
    Ki·ªÉm tra k·∫øt n·ªëi v·ªõi Ollama server
    """
    try:
        import requests
        response = requests.get(f"{ollama_url}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [model["name"] for model in models]
            
            if model_name in model_names:
                st.success(f"‚úÖ K·∫øt n·ªëi th√†nh c√¥ng! Model '{model_name}' ƒë√£ s·∫µn s√†ng.")
            else:
                st.warning(f"‚ö†Ô∏è K·∫øt n·ªëi th√†nh c√¥ng nh∆∞ng model '{model_name}' ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t.")
                st.info(f"Ch·∫°y l·ªánh: `ollama pull {model_name}` ƒë·ªÉ c√†i ƒë·∫∑t model.")
                st.info(f"C√°c model c√≥ s·∫µn: {', '.join(model_names) if model_names else 'Kh√¥ng c√≥'}")
        else:
            st.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Ollama server. Status code: {response.status_code}")
    except requests.exceptions.RequestException as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi v·ªõi Ollama server: {str(e)}")
        st.info("ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y. Ch·∫°y l·ªánh: `ollama serve`")

def handle_local_file(use_ollama_embeddings: bool):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng ch·ªçn t·∫£i file
    """
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    filename = st.text_input("T√™n file JSON:", "stack.json")
    directory = st.text_input("Th∆∞ m·ª•c ch·ª©a file:", "data")
    
    if st.button("üìÅ T·∫£i d·ªØ li·ªáu t·ª´ file"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
            try:
                seed_milvus(
                    'http://localhost:19530', 
                    collection_name, 
                    filename, 
                    directory, 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")

def handle_url_input(use_ollama_embeddings: bool):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng ch·ªçn crawl URL
    """
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test_live",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    url = st.text_input("Nh·∫≠p URL:", "https://tapchibitcoin.io")
    
    if st.button("üåê Crawl d·ªØ li·ªáu"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang crawl d·ªØ li·ªáu..."):
            try:
                seed_milvus_live(
                    url, 
                    'http://localhost:19530', 
                    collection_name, 
                    'stack-ai', 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"‚úÖ ƒê√£ crawl d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"‚ùå L·ªói khi crawl d·ªØ li·ªáu: {str(e)}")

# === GIAO DI·ªÜN CHAT CH√çNH ===
def setup_chat_interface(model_choice):
    st.title("üí¨ AI Assistant")
    
    # Caption ƒë·ªông theo model
    if model_choice == "Google Gemini":
        st.caption("üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† Google Gemini")
    else:
        st.caption("üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† Ollama")
    
    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}
        ]
        msgs.add_ai_message("Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")

    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

# === X·ª¨ L√ù TIN NH·∫ÆN NG∆Ø·ªúI D√ôNG ===
def handle_user_input(msgs, agent_executor):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng g·ª≠i tin nh·∫Øn:
    1. Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
    2. G·ªçi AI x·ª≠ l√Ω v√† tr·∫£ l·ªùi
    3. L∆∞u v√†o l·ªãch s·ª≠ chat
    """
    if prompt := st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Stack AI!"):
        # L∆∞u v√† hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        # X·ª≠ l√Ω v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            
            # L·∫•y l·ªãch s·ª≠ chat
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]

            try:
                # G·ªçi AI x·ª≠ l√Ω
                response = agent_executor.invoke(
                    {
                        "input": prompt,
                        "chat_history": chat_history
                    },
                    {"callbacks": [st_callback]}
                )

                # L∆∞u v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
                output = response["output"]
                st.session_state.messages.append({"role": "assistant", "content": output})
                msgs.add_ai_message(output)
                st.write(output)
                
            except Exception as e:
                error_msg = f"ƒê√£ x·∫£y ra l·ªói: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                msgs.add_ai_message(error_msg)

# === H√ÄM CH√çNH ===
def main():
    """
    H√†m ch√≠nh ƒëi·ªÅu khi·ªÉn lu·ªìng ch∆∞∆°ng tr√¨nh
    """
    initialize_app()
    model_choice, collection_to_query, ollama_model, ollama_url = setup_sidebar()
    msgs = setup_chat_interface(model_choice)
    
    # Kh·ªüi t·∫°o AI d·ª±a tr√™n l·ª±a ch·ªçn model ƒë·ªÉ tr·∫£ l·ªùi
    try:
        retriever = get_retriever(collection_to_query)
        
        if model_choice == "Google Gemini":
            agent_executor = get_llm_and_agent(retriever, "gemini")
        else:  # Ollama
            # C·∫≠p nh·∫≠t c·∫•u h√¨nh Ollama n·∫øu c·∫ßn
            if ollama_model and ollama_url:
                # C√≥ th·ªÉ th√™m logic ƒë·ªÉ truy·ªÅn c·∫•u h√¨nh Ollama
                pass
            agent_executor = get_llm_and_agent(retriever, "ollama")
        
        handle_user_input(msgs, agent_executor)
        
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {str(e)}")
        st.info("Vui l√≤ng ki·ªÉm tra:")
        st.info("1. Milvus server ƒëang ch·∫°y (localhost:19530)")
        st.info("2. API key ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng")
        st.info("3. Ollama server ƒëang ch·∫°y (n·∫øu s·ª≠ d·ª•ng Ollama)")

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    main()